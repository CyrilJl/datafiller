{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# To use this experimental feature, we need to explicitly ask for it:\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import BayesianRidge, Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from datafiller.multivariate import MultivariateImputer as DatafillerImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatafillerWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer = DatafillerImputer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # The imputer is stateless, so we don't need to do anything here.\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.imputer(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SPLITS = 5\n",
    "\n",
    "X_full, y_full = fetch_california_housing(return_X_y=True)\n",
    "# ~2k samples is enough for the purpose of the example.\n",
    "# Remove the following two lines for a slower run with different error bars.\n",
    "X_full = X_full[::10]\n",
    "y_full = y_full[::10]\n",
    "n_samples, n_features = X_full.shape\n",
    "\n",
    "# Add a single missing value to each row\n",
    "rng = np.random.RandomState(0)\n",
    "X_missing = X_full.copy()\n",
    "y_missing = y_full\n",
    "missing_samples = np.arange(n_samples)\n",
    "missing_features = rng.choice(n_features, n_samples, replace=True)\n",
    "X_missing[missing_samples, missing_features] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_for(X, y, imputer=None):\n",
    "    # We scale data before imputation and training a target estimator,\n",
    "    # because our target estimator and some of the imputers assume\n",
    "    # that the features have similar scales.\n",
    "    if imputer is None:\n",
    "        estimator = make_pipeline(RobustScaler(), BayesianRidge())\n",
    "    else:\n",
    "        estimator = make_pipeline(RobustScaler(), imputer, BayesianRidge())\n",
    "    return cross_val_score(\n",
    "        estimator, X, y, scoring=\"neg_mean_squared_error\", cv=N_SPLITS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the score on the entire dataset, with no missing values\n",
    "score_full_data = pd.DataFrame(\n",
    "    compute_score_for(X_full, y_full),\n",
    "    columns=[\"Full Data\"],\n",
    ")\n",
    "\n",
    "# Estimate the score after imputation (mean and median strategies)\n",
    "score_simple_imputer = pd.DataFrame()\n",
    "for strategy in (\"mean\", \"median\"):\n",
    "    score_simple_imputer[strategy] = compute_score_for(\n",
    "        X_missing, y_missing, SimpleImputer(strategy=strategy)\n",
    "    )\n",
    "\n",
    "# Estimate the score after iterative imputation of the missing values\n",
    "# with different estimators\n",
    "named_estimators = [\n",
    "    (\"Bayesian Ridge\", BayesianRidge()),\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        RandomForestRegressor(\n",
    "            # We tuned the hyperparameters of the RandomForestRegressor to get a good\n",
    "            # enough predictive performance for a restricted execution time.\n",
    "            n_estimators=5,\n",
    "            max_depth=10,\n",
    "            bootstrap=True,\n",
    "            max_samples=0.5,\n",
    "            n_jobs=2,\n",
    "            random_state=0,\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"Nystroem + Ridge\",\n",
    "        make_pipeline(\n",
    "            Nystroem(kernel=\"polynomial\", degree=2, random_state=0), Ridge(alpha=1e4)\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"k-NN\",\n",
    "        KNeighborsRegressor(n_neighbors=10),\n",
    "    ),\n",
    "]\n",
    "score_iterative_imputer = pd.DataFrame()\n",
    "# Iterative imputer is sensitive to the tolerance and\n",
    "# dependent on the estimator used internally.\n",
    "# We tuned the tolerance to keep this example run with limited computational\n",
    "# resources while not changing the results too much compared to keeping the\n",
    "# stricter default value for the tolerance parameter.\n",
    "tolerances = (1e-3, 1e-1, 1e-1, 1e-2)\n",
    "for (name, impute_estimator), tol in zip(named_estimators, tolerances):\n",
    "    score_iterative_imputer[name] = compute_score_for(\n",
    "        X_missing,\n",
    "        y_missing,\n",
    "        IterativeImputer(\n",
    "            random_state=0, estimator=impute_estimator, max_iter=40, tol=tol\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# Estimate the score after ImputeMultiVariate\n",
    "score_impute_multi_variate = pd.DataFrame()\n",
    "score_impute_multi_variate[\"Datafiller\"] = compute_score_for(\n",
    "    X_missing, y_missing, DatafillerWrapper()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat(\n",
    "    [score_full_data, score_simple_imputer, score_iterative_imputer, score_impute_multi_variate],\n",
    "    keys=[\"Original\", \"SimpleImputer\", \"IterativeImputer\", \"Datafiller\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# plot california housing results\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "means = -scores.mean()\n",
    "errors = scores.std()\n",
    "means.plot.barh(xerr=errors, ax=ax)\n",
    "ax.set_title(\"California Housing Regression with Different Imputation Methods\")\n",
    "ax.set_xlabel(\"MSE (smaller is better)\")\n",
    "ax.set_yticks(np.arange(means.shape[0]))\n",
    "ax.set_yticklabels([\" w/ \".join(label) for label in means.index.tolist()])\n",
    "plt.tight_layout(pad=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
